{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing `libraries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Web scraping` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cities dataframe (through webscraping)\n",
    "def get_cities(city_list):\n",
    "    # Initialize an empty list that will be filled with one dictionary of information per city\n",
    "    city_data = []\n",
    "\n",
    "    # Iterate through the list of cities to collect information\n",
    "    for city in city_list:\n",
    "        # Construct the Wikipedia URL for the city\n",
    "        url = f'https://en.wikipedia.org/wiki/{city}'\n",
    "\n",
    "        # Send a GET request and parse the HTML with BeautifulSoup\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        # Initialize an empty dictionary for the city's information\n",
    "        city_info = {}\n",
    "\n",
    "        # Extract relevant information from the HTML using selectors\n",
    "        city_info['city_name'] = soup.select(\".firstHeading\")[0].get_text()\n",
    "        city_info['country'] = soup.select(\".infobox-data\")[0].get_text()\n",
    "        city_info['latitude'] = soup.select(\".latitude\")[0].get_text()\n",
    "        city_info['longitude'] = soup.select(\".longitude\")[0].get_text()\n",
    "        city_info['website'] = soup.select_one('.infobox-label:-soup-contains(\"Website\")').find_next(class_='infobox-data').get_text()\n",
    "\n",
    "\n",
    "        # Check if elevation information is available on the page\n",
    "        elevation_info = soup.select_one('.infobox-label:-soup-contains(\"Elevation\")')\n",
    "        if elevation_info:\n",
    "            city_info['elevation'] = elevation_info.find_next(class_='infobox-data').get_text()\n",
    "\n",
    "        # Check if population information is available on the page\n",
    "        population_info = soup.select_one('th.infobox-header:-soup-contains(\"Population\")')\n",
    "        if population_info:\n",
    "            city_info['population'] = population_info.parent.find_next_sibling().find(string=re.compile(r'\\d+'))\n",
    "\n",
    "\n",
    "        # Add the city's dictionary to the list\n",
    "        city_data.append(city_info)\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    cities_df = pd.DataFrame(city_data)\n",
    "\n",
    "    # Fix formatting issues in latitude and longitude columns\n",
    "    cities_df['latitude'] = cities_df['latitude'].str.replace('°', '.').str.replace('′', '').str.replace('″', '')\n",
    "    cities_df['longitude'] = cities_df['longitude'].str.replace('°', '.').str.replace('′', '').str.replace('″', '')\n",
    "    \n",
    "    country_code = {\"Germany\": \"DE\",\n",
    "                \"United Kingdom\": \"UK\",\n",
    "                \"Spain\": \"ES\"}\n",
    "\n",
    "    cities_df[\"country_code\"] = \"\"\n",
    "    for i, country in enumerate(cities_df[\"country\"]):\n",
    "        cities_df.iloc[i,7]=country_code[country]\n",
    "\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return cities_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
